# DALL-E-Implementation
The implementation follows the approach described in the original paper of DALL-E by OpenAI. First, I developed the foundational architecture based on the model specifications outlined in the paper. This included defining both the discrete variational autoencoder (dVAE) and the Transformer for text-to-image generation. Next, I selected the NoCaps validation dataset which containing 45,000 image-caption pairs (4,500 unique images), ensuring proper preprocessing to align with the methodology presented in the paper. The training process was conducted in two sequential steps: (1) training the dVAE to encode and decode images into discrete token representations and (2) training the Transformer to map text descriptions to these image tokens. This two-stage training approach allows the model to effectively learn both a meaningful image representation and a robust text-to-image mapping.
